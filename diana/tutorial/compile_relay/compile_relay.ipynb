{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8deb5298-0296-4291-802e-dfc95c5b86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "        tvmc_compile_and_unpack, \n",
    "        relay_soma_conv2d,\n",
    "        create_demo_file, \n",
    "        parse_cli_options,\n",
    "        load_or_create_random_array\n",
    "        )\n",
    "import tvm\n",
    "import tvm.relay as relay\n",
    "import tvm.relay.transform as transform\n",
    "from tvm.driver.tvmc.model import TVMCModel\n",
    "from tvm.driver.tvmc.compiler import compile_model\n",
    "from tvm.relay.backend import Executor, Runtime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda74004-78eb-4557-973c-2a284d32a601",
   "metadata": {},
   "source": [
    "# Define a model in relay\n",
    "\n",
    "Below, you can find examples of manually constructed relay graphs with the TVM python relay API (https://tvm.apache.org/docs/reference/api/python/relay/index.html).\n",
    "\n",
    "This is an example graph with a single (compound) 2D convolution that is supported by the digital accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1f50f8-d24d-4a10-8c82-93584eab210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_single_layer():\n",
    "    input_shape = (1, 3, 32, 32)\n",
    "    weights_shape = (5, 3, 3, 3)\n",
    "    conv_channels = weights_shape[0]\n",
    "    shift_bits = 0\n",
    "    weights_name = \"weights\"\n",
    "    bias_name = \"bias\"\n",
    "    \n",
    "    # define variables\n",
    "    x = relay.var(\"input\", relay.TensorType(input_shape, 'int8'))\n",
    "    w = relay.var(weights_name, relay.TensorType(weights_shape, 'int8'))\n",
    "    b = relay.var(bias_name, relay.TensorType((conv_channels,), 'int32'))\n",
    "\n",
    "    # define weights and bias values\n",
    "    w_value = np.random.uniform(low=-10, high=10, size=weights_shape).astype(np.int8)\n",
    "    b_value = np.random.uniform(low=-10, high=10, size=conv_channels).astype(np.int32)\n",
    "    params = {weights_name: tvm.nd.array(w_value), bias_name: tvm.nd.array(b_value)}\n",
    "\n",
    "    # define diana composite convolution op\n",
    "    x = relay.qnn.op.conv2d(x, w, relay.const(0), relay.const(0), relay.const(1.0), relay.const(1.0), weights_shape[-2:], channels=conv_channels, padding=(1, 1))\n",
    "    x = relay.op.nn.bias_add(x, b)\n",
    "    x = relay.op.right_shift(x, relay.const(shift_bits))     # power-of-two quantization scale\n",
    "    x = relay.op.clip(x, a_min=-128, a_max=127)\n",
    "    x = relay.op.cast(x, 'int8')\n",
    "    x = relay.op.clip(x, a_min=0, a_max=127)                 # Relu\n",
    "\n",
    "    # create an IR module from the relay expression\n",
    "    mod = tvm.ir.IRModule()\n",
    "    mod = mod.from_expr(x)\n",
    "\n",
    "    return mod, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d97e1-0f1a-48fc-82de-a9277bb91427",
   "metadata": {},
   "source": [
    "A similar example with two convolutions. In order to make it more readable, it uses our relay_soma_conv2d utility function to construct a 2D convolution.\n",
    "When the attributes of a convolution don't match the supported attributes by the digital accelerator, the convolution will be offloaded to the CPU instead. In this case, a warning will be thrown to notify the user about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c1a872-8e89-4c9b-a10f-19180eb8ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_two_layers(strides_conv2=(1, 1)):\n",
    "    input_shape = (1, 3, 32, 32)\n",
    "    x = relay.var(\"input\", relay.TensorType(input_shape, 'int8'))\n",
    "\n",
    "    weights1_shape = (32, 3, 3, 3)\n",
    "    weights1 = load_or_create_random_array(\"weights1.npy\", weights1_shape, np.int8)\n",
    "    x, params1 = relay_soma_conv2d(x, 'conv1', weights1_shape, \n",
    "                                   weights1,\n",
    "                                   np.ones(weights1_shape[0]).astype(np.int32), \n",
    "                                   act=True, shift_bits=4)\n",
    "\n",
    "    weights2_shape = (8, 32, 3, 3)\n",
    "    weights2 = load_or_create_random_array(\"weights2.npy\", weights2_shape, np.int8)\n",
    "    x, params2 = relay_soma_conv2d(x, 'conv2', weights2_shape, \n",
    "                                   weights2,\n",
    "                                   np.ones(weights2_shape[0]).astype(np.int32),\n",
    "                                   strides=strides_conv2,\n",
    "                                   act=False, shift_bits=4)\n",
    "    params = params1\n",
    "    params.update(params2)\n",
    "\n",
    "    # create an IR module from the relay expression\n",
    "    mod = tvm.ir.IRModule()\n",
    "    mod = mod.from_expr(x)\n",
    "\n",
    "    return mod, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a6488-f22a-45c1-a7c3-23dc332e8eec",
   "metadata": {},
   "source": [
    "A similar example with two convolutions and a residual skip connection. Element-wise sums are currently offloaded to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4756af91-72f7-451a-947f-effd33e49fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_two_layers_and_residual():\n",
    "    input_shape = (1, 32, 32, 32)\n",
    "    x = relay.var(\"input\", relay.TensorType(input_shape, 'int8'))\n",
    "\n",
    "    weights1_shape = (16, 32, 3, 3)\n",
    "    weights1 = load_or_create_random_array(\"weights1.npy\", weights1_shape, np.int8)\n",
    "    y, params1 = relay_soma_conv2d(x, 'conv1', weights1_shape, \n",
    "                                   weights1,\n",
    "                                   np.ones(weights1_shape[0]).astype(np.int32), \n",
    "                                   act=True, shift_bits=4)\n",
    "\n",
    "    weights2_shape = (32, 16, 3, 3)\n",
    "    weights2 = load_or_create_random_array(\"weights2.npy\", weights2_shape, np.int8)\n",
    "    y, params2 = relay_soma_conv2d(y, 'conv2', weights2_shape, \n",
    "                                   weights2,\n",
    "                                   np.ones(weights2_shape[0]).astype(np.int32), \n",
    "                                   act=False, shift_bits=4)\n",
    "    x = relay.add(x, y)\n",
    "    \n",
    "    params = params1\n",
    "    params.update(params2)\n",
    "\n",
    "    # create an IR module from the relay expression\n",
    "    mod = tvm.ir.IRModule()\n",
    "    mod = mod.from_expr(x)\n",
    "\n",
    "    return mod, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cbc5f6-f573-4f65-b81b-a6c8808e84e7",
   "metadata": {},
   "source": [
    "## Create and show the relay graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dceb4570-0d26-4e49-9cdf-8659c6c6ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%input: Tensor[(1, 3, 32, 32), int8], %weights: Tensor[(5, 3, 3, 3), int8], %bias: Tensor[(5), int32]) {\n",
      "  %0 = qnn.conv2d(%input, %weights, 0, 0, 1f, 1f, padding=[1, 1, 1, 1], channels=5, kernel_size=[3, 3], out_dtype=\"int32\");\n",
      "  %1 = nn.bias_add(%0, %bias);\n",
      "  %2 = right_shift(%1, 0);\n",
      "  %3 = clip(%2, a_min=-128f, a_max=127f);\n",
      "  %4 = cast(%3, dtype=\"int8\");\n",
      "  clip(%4, a_min=0f, a_max=127f)\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod, params = create_model_single_layer()\n",
    "#mod, params = create_model_two_layers()\n",
    "#mod, params = create_model_two_layers((1, 2))\n",
    "#mod, params = create_model_two_layers_and_residual()\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f82a0-2b95-46d4-9f8f-3128090124a8",
   "metadata": {},
   "source": [
    "## Make it a TVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dee405f-ba81-4578-8993-b57b0903eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TVMCModel(mod, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236899a2-246c-4c09-b46b-d80221011e3c",
   "metadata": {},
   "source": [
    "# Compile the model to C code\n",
    "\n",
    "## Compilation options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ccbfd6-3d7f-44d0-a894-528b5d46e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"soma_dory, c\"      # send supported operations to the digital accelerator, generate C code for the CPU for all other operations\n",
    "#target = \"c\"                # generate C code for the CPU only\n",
    "\n",
    "fuse_layers = True           # enable/disable layer fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61307099-9e76-4556-a184-c4b385cc949e",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "Compile the TVM model and unpack the generated .tar file to a given build folder (build_path).\n",
    "\n",
    "The generated source code can be found in `build_folder/codegen/host/src`.\n",
    "\n",
    "The output contains a number of C files named `default_libX.c` with `X` a incremental number if more files are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1e859e-68e2-4cb2-b548-44b3116ede99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backend: Matching patterns from generated DORY ONNX to HW Nodes.\n",
      "\n",
      "Find One other solution, It will not work for real networks with multiple strides = 2\n",
      "\n",
      "Diana Backend: Adjusting Data Layout to HWC and CoutKCin.\n",
      "\n",
      "Updating memory occupation and MACs of tensors in layers\n",
      "\n",
      "Insert tiling parameters per layer inside graph nodes\n",
      "\n",
      "DORY Backend: Renaming Weights tensors.\n",
      "\n",
      "Mapping the layers files to their templates and copying the kernels associated.\n",
      "\n",
      "Generating weight string for tvmgen_default_soma_dory_main_0.\n",
      "def @main(%input: Tensor[(1, 3, 32, 32), int8] /* ty=Tensor[(1, 3, 32, 32), int8] */) -> Tensor[(1, 5, 32, 32), int8] {\n",
      "  %0 = reshape(%input, newshape=[768, 4]) /* ty=Tensor[(768, 4), int8] */;\n",
      "  %1 = reverse(%0, axis=1) /* ty=Tensor[(768, 4), int8] */;\n",
      "  %2 = reshape(%1, newshape=[1, 3, 32, 32]) /* ty=Tensor[(1, 3, 32, 32), int8] */;\n",
      "  %3 = @tvmgen_default_soma_dory_main_0(%2) /* ty=Tensor[(1, 5, 32, 32), int8] */;\n",
      "  %4 = reshape(%3, newshape=[1280, 4]) /* ty=Tensor[(1280, 4), int8] */;\n",
      "  %5 = reverse(%4, axis=1) /* ty=Tensor[(1280, 4), int8] */;\n",
      "  reshape(%5, newshape=[1, 5, 32, 32]) /* ty=Tensor[(1, 5, 32, 32), int8] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_soma_dory_main_0(%soma_dory_0_i0: Tensor[(1, 3, 32, 32), int8] /* ty=Tensor[(1, 3, 32, 32), int8] */, Inline=1, Compiler=\"soma_dory\", global_symbol=\"tvmgen_default_soma_dory_main_0\", Primitive=1) -> Tensor[(1, 5, 32, 32), int8] {\n",
      "  %11 = fn (%FunctionVar_0_0: Tensor[(1, 3, 32, 32), int8] /* ty=Tensor[(1, 3, 32, 32), int8] */, %FunctionVar_0_1: Tensor[(5, 3, 3, 3), int8] /* ty=Tensor[(5, 3, 3, 3), int8] */, %FunctionVar_0_2: Tensor[(5), int32] /* ty=Tensor[(5), int32] */, PartitionedFromPattern=\"qnn.conv2d_nn.bias_add_right_shift_clip_cast_clip_\", Composite=\"soma_dory.qnn_conv2d\") -> Tensor[(1, 5, 32, 32), int8] {\n",
      "    %6 = qnn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, 0 /* ty=int32 */, 0 /* ty=int32 */, 1f /* ty=float32 */, 1f /* ty=float32 */, padding=[1, 1, 1, 1], channels=5, kernel_size=[3, 3], out_dtype=\"int32\") /* ty=Tensor[(1, 5, 32, 32), int32] */;\n",
      "    %7 = nn.bias_add(%6, %FunctionVar_0_2) /* ty=Tensor[(1, 5, 32, 32), int32] */;\n",
      "    %8 = right_shift(%7, 0 /* ty=int32 */) /* ty=Tensor[(1, 5, 32, 32), int32] */;\n",
      "    %9 = clip(%8, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 5, 32, 32), int32] */;\n",
      "    %10 = cast(%9, dtype=\"int8\") /* ty=Tensor[(1, 5, 32, 32), int8] */;\n",
      "    clip(%10, a_min=0f, a_max=127f) /* ty=Tensor[(1, 5, 32, 32), int8] */\n",
      "  } /* ty=fn (Tensor[(1, 3, 32, 32), int8], Tensor[(5, 3, 3, 3), int8], Tensor[(5), int32]) -> Tensor[(1, 5, 32, 32), int8] */;\n",
      "  %11(%soma_dory_0_i0, meta[relay.Constant][0] /* ty=Tensor[(5, 3, 3, 3), int8] */, meta[relay.Constant][1] /* ty=Tensor[(5), int32] */) /* ty=Tensor[(1, 5, 32, 32), int8] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:23:46] /esat/sol1/users/jvandelm/gitlab-runner/builds/soma_compiler/tvm-fork/src/relay/backend/aot_executor_codegen.cc:497: CreateFuncCall: tvmgen_default_fused_reshape_reverse_reshape -> tir.tvm_check_return(0, -1, tir.call_extern(\"tvmgen_default_fused_reshape_reverse_reshape\", input_buffer_var, sid_1))\n",
      "\n",
      "[06:23:46] /esat/sol1/users/jvandelm/gitlab-runner/builds/soma_compiler/tvm-fork/src/relay/backend/aot_executor_codegen.cc:497: CreateFuncCall: tvmgen_default_soma_dory_main_0 -> tir.tvm_check_return(0, -1, tir.call_extern(\"tvmgen_default_soma_dory_main_0\", sid_1, sid_2))\n",
      "\n",
      "[06:23:46] /esat/sol1/users/jvandelm/gitlab-runner/builds/soma_compiler/tvm-fork/src/relay/backend/aot_executor_codegen.cc:497: CreateFuncCall: tvmgen_default_fused_reshape_reverse_reshape_1 -> tir.tvm_check_return(0, -1, tir.call_extern(\"tvmgen_default_fused_reshape_reverse_reshape_1\", sid_2, output_buffer_var))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvmc_compile_and_unpack(model, target=target, fuse_layers=fuse_layers, build_path='build')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4ed2d-b2c1-4290-a992-a24a05c3fca7",
   "metadata": {},
   "source": [
    "# Compile generated C code for DIANA\n",
    "## Copy the DORY runtime library files\n",
    "\n",
    "The generated model code makes calls to the DORY runtime library, which contains the microkernel implementation of all supported operations for the digital accelerator. We need to include these files into the build. A clone of DORY is installed in `/dory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e74f53f-fc4e-445a-9b16-ca41e0fdafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DORY_SRC_DIR=\"/dory\"\n",
    "DORY_DST_DIR=\"dory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1bad15b-dffa-48ae-9d54-f71fd7703d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $DORY_DST_DIR/include\n",
    "!mkdir -p $DORY_DST_DIR/src\n",
    "\n",
    "!cp $DORY_SRC_DIR/dory/Hardware_targets/Diana/Backend_Kernels/dory-hal/include/*.h $DORY_DST_DIR/include\n",
    "!cp $DORY_SRC_DIR/dory/Hardware_targets/Diana/Backend_Kernels/dory-hal/src/*.c $DORY_DST_DIR/src\n",
    "!cp $DORY_SRC_DIR/dory/Hardware_targets/Diana/Diana_TVM/Utils_files/*.h $DORY_DST_DIR/include\n",
    "!cp $DORY_SRC_DIR/dory/Hardware_targets/Diana/Diana_TVM/Utils_files/*.c $DORY_DST_DIR/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297acfc-e2b0-4749-a83a-bd4e7b477f66",
   "metadata": {},
   "source": [
    "## Generate a template application\n",
    "\n",
    "For this, we:\n",
    "* copy a few C files (including a malloc wrapper, prolfiling tools and some required header files)\n",
    "* Generate a main template function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d91c2f-969a-4e1c-aef6-b41bac84c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_DST_DIR=\"app\"\n",
    "APP_SRC_DIR=\"../../byoc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "959fa1f4-1a79-42dd-8c49-866c8c8940cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $APP_DST_DIR/src\n",
    "!mkdir -p $APP_DST_DIR/include\n",
    "\n",
    "!cp $APP_SRC_DIR/src/*.c $APP_DST_DIR/src\n",
    "!cp $APP_SRC_DIR/include/*.h $APP_DST_DIR/include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d07ca2-6987-4592-a123-1b1a82a2cdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating demo file: Inferring shapes and types...\n",
      "Creating demo file: Inferred shapes:\n",
      "\tinput (int8):\n",
      "\t [1 3 32 32]\n",
      "\toutput (int8):\n",
      "\t [1 5 32 32]\n"
     ]
    }
   ],
   "source": [
    "create_demo_file(mod, APP_DST_DIR + '/src/demo.c') # generate demo.c, the template main function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91138bb-25e5-40be-a88f-abd65ed6b744",
   "metadata": {},
   "source": [
    "## Cross-compile for DIANA\n",
    "\n",
    "Open a terminal and execute `make -f Makefile.pulprt clean all`. Note that we don't execute this command in the notebook due to the large list of environmental variables that need to be set in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3946b7-4c26-48a2-a3ef-8964bb3e1441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
